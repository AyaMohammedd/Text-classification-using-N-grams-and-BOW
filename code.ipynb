{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d74d0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d8051c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ayamohammed/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ayamohammed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ayamohammed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ayamohammed/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dfd6b703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1  https://www.huffpost.com/entry/american-airlin...   \n",
       "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3  https://www.huffpost.com/entry/funniest-parent...   \n",
       "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "\n",
       "                                            headline   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                   short_description               authors  \\\n",
       "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
       "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
       "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
       "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
       "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
       "\n",
       "        date  \n",
       "0 2022-09-23  \n",
       "1 2022-09-23  \n",
       "2 2022-09-23  \n",
       "3 2022-09-23  \n",
       "4 2022-09-22  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the JSON file containing news articles data\n",
    "df = pd.read_json(\"data/News_Category_Dataset_v3.json\", lines=True)\n",
    "\n",
    "# Displaying the first few rows of the loaded dataframe(understanding the structure and content of the data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6657e709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline   category\n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS\n",
       "1       American Airlines Flyer Charged, Banned For Li...  U.S. NEWS\n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY\n",
       "3       The Funniest Tweets From Parents This Week (Se...  PARENTING\n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS\n",
       "...                                                   ...        ...\n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...       TECH\n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...     SPORTS\n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...     SPORTS\n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...     SPORTS\n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...     SPORTS\n",
       "\n",
       "[209527 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting only the 'headline' and 'category' columns for our text classification task\n",
    "df = df[[\"headline\", \"category\"]]\n",
    "\n",
    "# Displaying the filtered dataframe to verify the changes\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d46d27d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is :  (209527, 2)\n",
      "The number of rows is :  209527\n",
      "The number of columns is :  2\n",
      "The unique categories are :  ['U.S. NEWS' 'COMEDY' 'PARENTING' 'WORLD NEWS' 'CULTURE & ARTS' 'TECH'\n",
      " 'SPORTS' 'ENTERTAINMENT' 'POLITICS' 'WEIRD NEWS' 'ENVIRONMENT'\n",
      " 'EDUCATION' 'CRIME' 'SCIENCE' 'WELLNESS' 'BUSINESS' 'STYLE & BEAUTY'\n",
      " 'FOOD & DRINK' 'MEDIA' 'QUEER VOICES' 'HOME & LIVING' 'WOMEN'\n",
      " 'BLACK VOICES' 'TRAVEL' 'MONEY' 'RELIGION' 'LATINO VOICES' 'IMPACT'\n",
      " 'WEDDINGS' 'COLLEGE' 'PARENTS' 'ARTS & CULTURE' 'STYLE' 'GREEN' 'TASTE'\n",
      " 'HEALTHY LIVING' 'THE WORLDPOST' 'GOOD NEWS' 'WORLDPOST' 'FIFTY' 'ARTS'\n",
      " 'DIVORCE']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shape is : \",df.shape)\n",
    "print(\"The number of rows is : \",df.shape[0])\n",
    "print(\"The number of columns is : \",df.shape[1])\n",
    "print(\"The unique categories are : \",df['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "22868c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "POLITICS          35602\n",
       "WELLNESS          17945\n",
       "ENTERTAINMENT     17362\n",
       "TRAVEL             9900\n",
       "STYLE & BEAUTY     9814\n",
       "PARENTING          8791\n",
       "HEALTHY LIVING     6694\n",
       "QUEER VOICES       6347\n",
       "FOOD & DRINK       6340\n",
       "BUSINESS           5992\n",
       "COMEDY             5400\n",
       "SPORTS             5077\n",
       "BLACK VOICES       4583\n",
       "HOME & LIVING      4320\n",
       "PARENTS            3955\n",
       "THE WORLDPOST      3664\n",
       "WEDDINGS           3653\n",
       "WOMEN              3572\n",
       "CRIME              3562\n",
       "IMPACT             3484\n",
       "DIVORCE            3426\n",
       "WORLD NEWS         3299\n",
       "MEDIA              2944\n",
       "WEIRD NEWS         2777\n",
       "GREEN              2622\n",
       "WORLDPOST          2579\n",
       "RELIGION           2577\n",
       "STYLE              2254\n",
       "SCIENCE            2206\n",
       "TECH               2104\n",
       "TASTE              2096\n",
       "MONEY              1756\n",
       "ARTS               1509\n",
       "ENVIRONMENT        1444\n",
       "FIFTY              1401\n",
       "GOOD NEWS          1398\n",
       "U.S. NEWS          1377\n",
       "ARTS & CULTURE     1339\n",
       "COLLEGE            1144\n",
       "LATINO VOICES      1130\n",
       "CULTURE & ARTS     1074\n",
       "EDUCATION          1014\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is Imbalance Impact\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4fb6fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of samples in the majority class\n",
    "max_samples = df['category'].value_counts().max()\n",
    "\n",
    "# Create an empty list to store upsampled dataframes\n",
    "df_upsampled = pd.DataFrame()\n",
    "\n",
    "# Loop through each category and upsample\n",
    "for category in df['category'].unique():\n",
    "    # Get the samples for the current category\n",
    "    category_samples = df[df['category'] == category]\n",
    "    \n",
    "    # Calculate how many samples to generate (upsample to match the majority class)\n",
    "    n_samples = max_samples - len(category_samples)\n",
    "    \n",
    "    if n_samples > 0:  # Only upsample if the category has fewer samples than the majority\n",
    "        # Perform upsampling with replacement\n",
    "        upsampled = resample(category_samples,\n",
    "                           replace=True,      # Sample with replacement\n",
    "                           n_samples=n_samples,  # Number of samples to generate\n",
    "                           random_state=42) \n",
    "        \n",
    "        # Combine the original and upsampled data\n",
    "        df_upsampled = pd.concat([df_upsampled, category_samples, upsampled])\n",
    "    else:\n",
    "        # If the category already has the maximum number of samples, just add it\n",
    "        df_upsampled = pd.concat([df_upsampled, category_samples])\n",
    "\n",
    "\n",
    "# Shuffle the dataset to mix up the order of samples\n",
    "df_balanced = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7a299501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset class distribution:\n",
      "category\n",
      "STYLE             35602\n",
      "TRAVEL            35602\n",
      "COLLEGE           35602\n",
      "WOMEN             35602\n",
      "POLITICS          35602\n",
      "QUEER VOICES      35602\n",
      "PARENTING         35602\n",
      "HEALTHY LIVING    35602\n",
      "BUSINESS          35602\n",
      "EDUCATION         35602\n",
      "FOOD & DRINK      35602\n",
      "WEDDINGS          35602\n",
      "WORLDPOST         35602\n",
      "ARTS & CULTURE    35602\n",
      "ENTERTAINMENT     35602\n",
      "WELLNESS          35602\n",
      "RELIGION          35602\n",
      "DIVORCE           35602\n",
      "LATINO VOICES     35602\n",
      "BLACK VOICES      35602\n",
      "THE WORLDPOST     35602\n",
      "STYLE & BEAUTY    35602\n",
      "SPORTS            35602\n",
      "CRIME             35602\n",
      "PARENTS           35602\n",
      "FIFTY             35602\n",
      "SCIENCE           35602\n",
      "WORLD NEWS        35602\n",
      "GREEN             35602\n",
      "GOOD NEWS         35602\n",
      "TASTE             35602\n",
      "MONEY             35602\n",
      "CULTURE & ARTS    35602\n",
      "HOME & LIVING     35602\n",
      "ARTS              35602\n",
      "ENVIRONMENT       35602\n",
      "TECH              35602\n",
      "MEDIA             35602\n",
      "COMEDY            35602\n",
      "WEIRD NEWS        35602\n",
      "IMPACT            35602\n",
      "U.S. NEWS         35602\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original dataset shape: (209527, 2)\n",
      "Balanced dataset shape: (1495284, 2)\n"
     ]
    }
   ],
   "source": [
    "# Verify the new class distribution\n",
    "print(\"Balanced dataset class distribution:\")\n",
    "print(df_balanced['category'].value_counts())\n",
    "\n",
    "# Check the new shape of the dataset\n",
    "print(\"\\nOriginal dataset shape:\", df.shape)\n",
    "print(\"Balanced dataset shape:\", df_balanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1114320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a single text string by performing the following steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove special characters and numbers\n",
    "    3. Tokenize the text\n",
    "    4. Remove stopwords\n",
    "    5. Lemmatize words\n",
    "    6. Join tokens back into a single string\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Remove single character tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    \n",
    "    # Join tokens back to string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96e3ada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text preprocessing...\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the 'headline' column\n",
    "print(\"Starting text preprocessing...\")\n",
    "df_balanced['processed_headline'] = df_balanced['headline'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5d9af5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original vs Processed Text Examples:\n",
      "\n",
      "Original: Bad Beard Days Happen, So Here's How To Handle Them\n",
      "Processed: bad beard day happen here handle\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Original: Michelle Kwan Wedding Dress: Olympic Skater Wants Vera Wang To Make Her Wedding Gown\n",
      "Processed: michelle kwan wedding dress olympic skater want vera wang make wedding gown\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Original: Dad Life Is Totally Metal\n",
      "Processed: dad life totally metal\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show before and after examples\n",
    "print(\"\\nOriginal vs Processed Text Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df_balanced['headline'].iloc[i]}\")\n",
    "    print(f\"Processed: {df_balanced['processed_headline'].iloc[i]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6542c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'headline' column\n",
    "df_balanced.drop(columns=['headline'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8a00e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category to Number Mapping:\n",
      "ARTS: 0\n",
      "ARTS & CULTURE: 1\n",
      "BLACK VOICES: 2\n",
      "BUSINESS: 3\n",
      "COLLEGE: 4\n",
      "COMEDY: 5\n",
      "CRIME: 6\n",
      "CULTURE & ARTS: 7\n",
      "DIVORCE: 8\n",
      "EDUCATION: 9\n",
      "ENTERTAINMENT: 10\n",
      "ENVIRONMENT: 11\n",
      "FIFTY: 12\n",
      "FOOD & DRINK: 13\n",
      "GOOD NEWS: 14\n",
      "GREEN: 15\n",
      "HEALTHY LIVING: 16\n",
      "HOME & LIVING: 17\n",
      "IMPACT: 18\n",
      "LATINO VOICES: 19\n",
      "MEDIA: 20\n",
      "MONEY: 21\n",
      "PARENTING: 22\n",
      "PARENTS: 23\n",
      "POLITICS: 24\n",
      "QUEER VOICES: 25\n",
      "RELIGION: 26\n",
      "SCIENCE: 27\n",
      "SPORTS: 28\n",
      "STYLE: 29\n",
      "STYLE & BEAUTY: 30\n",
      "TASTE: 31\n",
      "TECH: 32\n",
      "THE WORLDPOST: 33\n",
      "TRAVEL: 34\n",
      "U.S. NEWS: 35\n",
      "WEDDINGS: 36\n",
      "WEIRD NEWS: 37\n",
      "WELLNESS: 38\n",
      "WOMEN: 39\n",
      "WORLD NEWS: 40\n",
      "WORLDPOST: 41\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Create a new column with numerical labels\n",
    "df_balanced['category_encoded'] = label_encoder.fit_transform(df_balanced['category'])\n",
    "\n",
    "# Create a mapping dictionary for reference\n",
    "category_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "# Display the mapping\n",
    "print(\"Category to Number Mapping:\")\n",
    "for category, num in sorted(category_mapping.items(), key=lambda x: x[1]):\n",
    "    print(f\"{category}: {num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4087b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows with encoded categories:\n",
      "   category  category_encoded\n",
      "0     STYLE                29\n",
      "1  WEDDINGS                36\n",
      "2   PARENTS                23\n",
      "3     FIFTY                12\n",
      "4   SCIENCE                27\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst few rows with encoded categories:\")\n",
    "print(df_balanced[['category', 'category_encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e4039",
   "metadata": {},
   "source": [
    "When you use **stratify**, you're telling train_test_split to:\n",
    "\n",
    "Look at the original proportions of each category in your data\n",
    "\n",
    "Maintain those exact same proportions when creating both the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "70869d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_balanced['processed_headline'], df_balanced['category_encoded'], test_size=0.2, random_state=42,stratify=df_balanced['category_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "44da094b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      " category_encoded\n",
      "29    0.02381\n",
      "34    0.02381\n",
      "4     0.02381\n",
      "39    0.02381\n",
      "24    0.02381\n",
      "25    0.02381\n",
      "22    0.02381\n",
      "16    0.02381\n",
      "3     0.02381\n",
      "9     0.02381\n",
      "13    0.02381\n",
      "36    0.02381\n",
      "41    0.02381\n",
      "1     0.02381\n",
      "10    0.02381\n",
      "38    0.02381\n",
      "26    0.02381\n",
      "8     0.02381\n",
      "19    0.02381\n",
      "2     0.02381\n",
      "33    0.02381\n",
      "30    0.02381\n",
      "28    0.02381\n",
      "6     0.02381\n",
      "23    0.02381\n",
      "12    0.02381\n",
      "27    0.02381\n",
      "40    0.02381\n",
      "15    0.02381\n",
      "14    0.02381\n",
      "31    0.02381\n",
      "21    0.02381\n",
      "7     0.02381\n",
      "17    0.02381\n",
      "0     0.02381\n",
      "11    0.02381\n",
      "32    0.02381\n",
      "20    0.02381\n",
      "5     0.02381\n",
      "37    0.02381\n",
      "18    0.02381\n",
      "35    0.02381\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Train distribution:\n",
      " category_encoded\n",
      "19    0.023810\n",
      "15    0.023810\n",
      "16    0.023810\n",
      "35    0.023810\n",
      "33    0.023810\n",
      "8     0.023810\n",
      "17    0.023810\n",
      "12    0.023810\n",
      "39    0.023810\n",
      "5     0.023810\n",
      "40    0.023810\n",
      "24    0.023810\n",
      "30    0.023810\n",
      "11    0.023810\n",
      "27    0.023810\n",
      "13    0.023810\n",
      "36    0.023810\n",
      "29    0.023810\n",
      "25    0.023810\n",
      "6     0.023810\n",
      "4     0.023810\n",
      "9     0.023810\n",
      "26    0.023810\n",
      "0     0.023810\n",
      "1     0.023810\n",
      "23    0.023809\n",
      "34    0.023809\n",
      "37    0.023809\n",
      "18    0.023809\n",
      "2     0.023809\n",
      "32    0.023809\n",
      "3     0.023809\n",
      "20    0.023809\n",
      "28    0.023809\n",
      "14    0.023809\n",
      "7     0.023809\n",
      "31    0.023809\n",
      "22    0.023809\n",
      "10    0.023809\n",
      "41    0.023809\n",
      "21    0.023809\n",
      "38    0.023809\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test distribution:\n",
      " category_encoded\n",
      "31    0.023812\n",
      "21    0.023812\n",
      "20    0.023812\n",
      "41    0.023812\n",
      "7     0.023812\n",
      "28    0.023812\n",
      "32    0.023812\n",
      "3     0.023812\n",
      "22    0.023812\n",
      "23    0.023812\n",
      "10    0.023812\n",
      "37    0.023812\n",
      "2     0.023812\n",
      "18    0.023812\n",
      "34    0.023812\n",
      "38    0.023812\n",
      "14    0.023812\n",
      "27    0.023808\n",
      "0     0.023808\n",
      "13    0.023808\n",
      "35    0.023808\n",
      "6     0.023808\n",
      "11    0.023808\n",
      "15    0.023808\n",
      "40    0.023808\n",
      "33    0.023808\n",
      "39    0.023808\n",
      "5     0.023808\n",
      "8     0.023808\n",
      "19    0.023808\n",
      "25    0.023808\n",
      "1     0.023808\n",
      "26    0.023808\n",
      "9     0.023808\n",
      "29    0.023808\n",
      "24    0.023808\n",
      "30    0.023808\n",
      "4     0.023808\n",
      "36    0.023808\n",
      "12    0.023808\n",
      "17    0.023808\n",
      "16    0.023808\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Original distribution:\\n\", df_balanced['category_encoded'].value_counts(normalize=True))\n",
    "print(\"\\nTrain distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest distribution:\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fff0d891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96      7120\n",
      "           1       0.97      0.99      0.98      7120\n",
      "           2       0.89      0.89      0.89      7121\n",
      "           3       0.84      0.79      0.81      7121\n",
      "           4       0.94      0.98      0.96      7120\n",
      "           5       0.88      0.83      0.85      7120\n",
      "           6       0.94      0.96      0.95      7120\n",
      "           7       0.97      0.99      0.98      7121\n",
      "           8       0.92      0.92      0.92      7120\n",
      "           9       0.93      0.98      0.95      7120\n",
      "          10       0.88      0.69      0.77      7121\n",
      "          11       0.96      0.99      0.97      7120\n",
      "          12       0.82      0.91      0.86      7120\n",
      "          13       0.87      0.87      0.87      7120\n",
      "          14       0.94      0.98      0.96      7121\n",
      "          15       0.92      0.95      0.93      7120\n",
      "          16       0.69      0.66      0.67      7120\n",
      "          17       0.93      0.95      0.94      7120\n",
      "          18       0.82      0.81      0.82      7121\n",
      "          19       0.97      0.99      0.98      7120\n",
      "          20       0.93      0.96      0.95      7121\n",
      "          21       0.90      0.97      0.94      7121\n",
      "          22       0.74      0.67      0.70      7121\n",
      "          23       0.79      0.80      0.80      7121\n",
      "          24       0.83      0.60      0.69      7120\n",
      "          25       0.94      0.89      0.91      7120\n",
      "          26       0.93      0.95      0.94      7120\n",
      "          27       0.94      0.97      0.95      7120\n",
      "          28       0.95      0.95      0.95      7121\n",
      "          29       0.89      0.96      0.92      7120\n",
      "          30       0.92      0.86      0.89      7120\n",
      "          31       0.89      0.93      0.91      7121\n",
      "          32       0.93      0.98      0.96      7121\n",
      "          33       0.92      0.92      0.92      7120\n",
      "          34       0.88      0.86      0.87      7121\n",
      "          35       0.96      1.00      0.98      7120\n",
      "          36       0.94      0.95      0.94      7120\n",
      "          37       0.93      0.96      0.94      7121\n",
      "          38       0.62      0.58      0.60      7121\n",
      "          39       0.83      0.85      0.84      7120\n",
      "          40       0.93      0.94      0.94      7120\n",
      "          41       0.89      0.93      0.91      7121\n",
      "\n",
      "    accuracy                           0.89    299057\n",
      "   macro avg       0.89      0.89      0.89    299057\n",
      "weighted avg       0.89      0.89      0.89    299057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_BOW = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 1))),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "clf_BOW.fit(X_train, y_train)\n",
    "y_pred = clf_BOW.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78059816",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_BI = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "clf_BI.fit(X_train, y_train)\n",
    "y_pred = clf_BI.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88324f2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m clf_TRI = Pipeline([\n\u001b[32m      2\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mvectorizer\u001b[39m\u001b[33m'\u001b[39m, CountVectorizer(ngram_range=(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m))),\n\u001b[32m      3\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m'\u001b[39m, LogisticRegression(max_iter=\u001b[32m1000\u001b[39m, random_state=\u001b[32m42\u001b[39m))\n\u001b[32m      4\u001b[39m ])\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m y_pred = clf.predict(X_test)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/sklearn/pipeline.py:662\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    657\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    658\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    659\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    660\u001b[39m             all_params=params,\n\u001b[32m    661\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1350\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1348\u001b[39m     n_threads = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1350\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m fold_coefs_, _, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n\u001b[32m   1376\u001b[39m \u001b[38;5;28mself\u001b[39m.n_iter_ = np.asarray(n_iter_, dtype=np.int32)[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/joblib/parallel.py:1985\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1983\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1984\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1985\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/joblib/parallel.py:1913\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1911\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1913\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/sklearn/utils/parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:451\u001b[39m, in \u001b[36m_logistic_regression_path\u001b[39m\u001b[34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[39m\n\u001b[32m    447\u001b[39m l2_reg_strength = \u001b[32m1.0\u001b[39m / (C * sw_sum)\n\u001b[32m    448\u001b[39m iprint = [-\u001b[32m1\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m101\u001b[39m][\n\u001b[32m    449\u001b[39m     np.searchsorted(np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m]), verbose)\n\u001b[32m    450\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m opt_res = \u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxiter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaxls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default is 20\u001b[39;49;00m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43miprint\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgtol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mftol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m n_iter_i = _check_optimize_result(\n\u001b[32m    466\u001b[39m     solver,\n\u001b[32m    467\u001b[39m     opt_res,\n\u001b[32m    468\u001b[39m     max_iter,\n\u001b[32m    469\u001b[39m     extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[32m    470\u001b[39m )\n\u001b[32m    471\u001b[39m w0, loss = opt_res.x, opt_res.fun\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/optimize/_minimize.py:738\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    735\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    736\u001b[39m                              **options)\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    741\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    742\u001b[39m                         **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:441\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[39m\n\u001b[32m    433\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    434\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    439\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    440\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    443\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    444\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:344\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    343\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:295\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n\u001b[32m    297\u001b[39m             \u001b[38;5;28mself\u001b[39m._lowest_x = \u001b[38;5;28mself\u001b[39m.x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:21\u001b[39m, in \u001b[36m_wrapper_fun.<locals>.wrapped\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     17\u001b[39m ncalls[\u001b[32m0\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m fx = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isscalar(fx):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/optimize/_optimize.py:80\u001b[39m, in \u001b[36mMemoizeJac.__call__\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, *args):\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/optimize/_optimize.py:74\u001b[39m, in \u001b[36mMemoizeJac._compute_if_needed\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(x == \u001b[38;5;28mself\u001b[39m.x) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.x = np.asarray(x).copy()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     fg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.jac = fg[\u001b[32m1\u001b[39m]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._value = fg[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/sklearn/linear_model/_linear_loss.py:336\u001b[39m, in \u001b[36mLinearModelLoss.loss_gradient\u001b[39m\u001b[34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[39m\n\u001b[32m    334\u001b[39m grad = np.empty((n_classes, n_dof), dtype=weights.dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# grad_pointwise.shape = (n_samples, n_classes)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m grad[:, :n_features] = \u001b[43mgrad_pointwise\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m + l2_reg_strength * weights\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit_intercept:\n\u001b[32m    338\u001b[39m     grad[:, -\u001b[32m1\u001b[39m] = grad_pointwise.sum(axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/sparse/_base.py:738\u001b[39m, in \u001b[36m_spbase.__rmatmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    736\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScalar operands are not allowed, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    737\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33muse \u001b[39m\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rmatmul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/sparse/_base.py:719\u001b[39m, in \u001b[36m_spbase._rmatmul_dispatch\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m    718\u001b[39m     tr = np.asarray(other).transpose()\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_matmul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/sparse/_base.py:624\u001b[39m, in \u001b[36m_spbase._matmul_dispatch\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    622\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape(M, \u001b[32m1\u001b[39m)\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m other.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other.shape[\u001b[32m0\u001b[39m] == N:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_matmul_multivector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    627\u001b[39m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[32m    628\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mul_scalar(other)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/churn_prediction/lib/python3.12/site-packages/scipy/sparse/_compressed.py:539\u001b[39m, in \u001b[36m_cs_matrix._matmul_multivector\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[32m    538\u001b[39m fn = \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m.format + \u001b[33m'\u001b[39m\u001b[33m_matvecs\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m   \u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape((n_vecs,))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "clf_TRI = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "clf_TRI.fit(X_train, y_train)\n",
    "y_pred = clf_TRI.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52daabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (churn_prediction)",
   "language": "python",
   "name": "churn_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
